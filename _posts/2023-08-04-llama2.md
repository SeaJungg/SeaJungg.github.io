---
layout: post
title: "autotraion-advance 패키지로 LLaMA2 finetuning하기"
category: done
---

준비
-
- https://ai.meta.com/resources/models-and-libraries/llama-downloads/ 에 들어가 Request Access form을 작성하고 제출한다.
- 메일로 다운로드 링크가 꽤 빨리(1시간 이내?) 온다. autotrain 패키지 내에서 허깅페이스 모델을 지정할 수 있어서 굳이 다운로드하지는 않았다.
- 위의 사이트에서 제출한 사람에 한해 https://huggingface.co/meta-llama/Llama-2-7b-hf 에 들어가 request를 신청하면 접근권한을 준다. 3시간 정도 걸렸다.
- 허깅페이스의 https://huggingface.co/settings/tokens 에 들어가 write권한으로 토큰을 만든다.
    

장비 셋팅
-
- Windows 11 + wsl 환경에서 진행했다.
- 메모리 128기가, GPU 24기가
    

실행
-
- https://www.youtube.com/watch?v=3fsn19OI_C8&t=114s 이 영상을 참고했다.
- autotrain-advanced 패키지를 이용하면 쉽게 트레이닝할 수 있다.
```
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
git lfs install
sudo apt-get install -y libjpeg8-dev zlib1g-dev libtiff5-dev libfreetype6-dev liblcms2-dev libopenjp2-7-dev libwebp-dev
```
```
pip install autotrain-advanced
```
- 아마 잘 안 될 수 있다. https://github.com/huggingface/autotrain-advanced/issues 를 보고 적절히 해결하길 바람..
- 아래 커맨드를 실행하면 알아서 GPU를 끌어다 쓰면서 잘 트레이닝을 시키기 시작한다.
```
autotrain llm --train --project_name my_llama_dolly_ko --model meta-llama/Llama-2-7b-chat-hf --data_path . --use_peft --use_int4 --learning_rate 2e-4 --train_batch_size 12 --num_train_epochs 3 --trainer sft --push_to_hub --repo_id seajungg/my_llama_dolly_ko
```
각 파라미터의 의미는 다음과 같다.
1. 'autotrain lllm': 언어 모델(llm)을 위해 AutoTrain 라이브러리를 호출합니다.
2. '--train': 모델을 훈련시키겠다는 뜻입니다.
3. '--project_name my-llm': 프로젝트 이름을 'my-llm'으로 설정하기 위해서입니다.
4. '--model meta-lama/Llama-2-7b-chat-hf': 메타 라마 라이브러리에서 사용할 모델을 지정합니다.
5. '--data_path .': 훈련 데이터가 있는 위치입니다. 'disclipse'는 현재 디렉터리에 있다는 것을 의미합니다.
6. '--use_peft': 표현을 위해 Port-Extensible Feature Tensor를 사용합니다.
7. '--use_int4': 모델 매개 변수를 저장할 때 4비트 정수를 사용합니다.
8. '--learning_rate 2e-4': 학습률을 0.0002로 설정합니다.
9. '--train_batch_size 12': 교육 시 배치 크기를 12로 설정하며 배치 크기는 모델을 한 번에 통과시킬 샘플의 개수입니다.
10. '--num_train_epocs 3': 학습 알고리즘이 전체 학습 데이터 세트를 통해 작동하는 횟수(이 경우 3으로 설정됨).
11. '--trainers sft': 사용할 트레이너를 지정합니다. 이 경우 sft(스코틀랜드의 최종 훈련생, 오토트레인 라이브러리의 특정 트레이너이거나 프로젝트를 위해 특별히 구성된 트레이너일 수 있음).
사용 중인 기계학습 라이브러리에 따라 약간의 차이가 있을 수 있습니다.

    

결과
-
- 판례데이터 8만5천건 기준 20시간이 걸렸다. instruction에 판시사항을 넣고, output에 판결요지를 넣었다.
- 20시간 후, 나는 쓰레기 모델을 만들었음을 알게 됐다.
```
from transformers import AutoTokenizer, LlamaForCausalLM, LlamaTokenizer
import torch

model = LlamaForCausalLM.from_pretrained("/home/nexusai/2023/7/tuning/data/my-llm")
tokenizer = LlamaTokenizer.from_pretrained("/home/nexusai/2023/7/tuning/data/my-llm") 

#prompt = "Hey, are you conscious? Can you talk to me?"
prompt = "노조활동을 구실로 정상적인 근무를 해태한 자에 대한 징계해고처분의 당부"
inputs = tokenizer(prompt, return_tensors="pt")

generate_ids = model.generate(inputs.input_ids, max_length=3000)
result = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
print(result)
```
- "Hey, are you conscious? Can you talk to me?" 라는 질문에도 대답을 못한다.
- 트레인 데이터 중 하나인 "노조활동을 구실로 정상적인 근무를 해태한 자에 대한 징계해고처분의 당부"를 넣으면 아예 대답이 안나온다.

결론
-
- 데이터가 문제인가 싶어 https://huggingface.co/datasets/nlpai-lab/databricks-dolly-15k-ko 이걸로 다시 돌려보고 있다.
- 그냥 https://huggingface.co/beomi/llama-2-ko-7b 이게 잘 완료되기를 기다리는 것도 나쁘지 않을 것 같단 생각이 든다.